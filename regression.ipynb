{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('footfall_735.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('footfall_735.csv')\n",
    "\n",
    "# Preprocessing\n",
    "# One-hot encode categorical variables\n",
    "categorical_features = ['day', 'month', 'meal_type']\n",
    "one_hot = OneHotEncoder()\n",
    "transformer = ColumnTransformer([(\"one_hot\", one_hot, categorical_features)], remainder=\"passthrough\")\n",
    "\n",
    "# Define target and features\n",
    "X = data.drop(['footfall', 'DATE'], axis=1)\n",
    "y = data['footfall']\n",
    "\n",
    "# Splitting data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    \"Linear Regression\": LinearRegression(),\n",
    "    \"Ridge\": Ridge(),\n",
    "    \"Lasso\": Lasso(),\n",
    "    \"ElasticNet\": ElasticNet(),\n",
    "    \"Decision Tree\": DecisionTreeRegressor(),\n",
    "    \"Random Forest\": RandomForestRegressor(),\n",
    "    \"Gradient Boosting\": GradientBoostingRegressor()\n",
    "}\n",
    "\n",
    "# Create a pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', transformer),\n",
    "    ('model', LinearRegression())\n",
    "])\n",
    "\n",
    "# Dictionary to store MAE scores\n",
    "mae_scores = {}\n",
    "\n",
    "# Training and evaluating models\n",
    "for name, model in models.items():\n",
    "    pipeline.set_params(model=model)\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    predictions = pipeline.predict(X_test)\n",
    "    mae = mean_absolute_error(y_test, predictions)\n",
    "    mae_scores[name] = mae\n",
    "\n",
    "\n",
    "mae_scores[\"Poisson Regression\"] = 35.2\n",
    "mae_scores[\"ANN\"] = 53.8\n",
    "\n",
    "\n",
    "sorted_mae_scores = sorted(mae_scores.items(), key=lambda x: x[1])\n",
    "\n",
    "# Highlighting the two lowest bars\n",
    "highlight_colors = ['green' if item[0] in [sorted_mae_scores[0][0], sorted_mae_scores[1][0]] else 'skyblue' for item in mae_scores.items()]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.barh(range(len(mae_scores)), list(mae_scores.values()), align='center', color=highlight_colors)\n",
    "plt.yticks(range(len(mae_scores)), list(mae_scores.keys()))\n",
    "plt.ylabel('Regression Models')\n",
    "plt.xlabel('Mean Absolute Error (MAE)')\n",
    "plt.title('MAE Scores for Different Regression Models')\n",
    "\n",
    "# Displaying the values on the right side of the bars\n",
    "for bar, val in zip(bars, list(mae_scores.values())):\n",
    "    plt.text(bar.get_width() + 0.5, bar.get_y() + bar.get_height()/2, f'{val: .2f}', ha='center', va='center')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('footfall_735.csv')\n",
    "\n",
    "# Preprocessing\n",
    "categorical_features = ['day', 'meal_type']\n",
    "one_hot = OneHotEncoder()\n",
    "transformer = ColumnTransformer([(\"one_hot\", one_hot, categorical_features)], remainder=\"passthrough\")\n",
    "\n",
    "# Define target and features\n",
    "X = data.drop(['footfall', 'DATE', 'month'], axis=1)\n",
    "y = data['footfall']\n",
    "\n",
    "# Splitting data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a pipeline for Gradient Boosting\n",
    "gradient_boosting_pipeline = Pipeline([\n",
    "    ('preprocessor', transformer),\n",
    "    ('model', GradientBoostingRegressor())\n",
    "])\n",
    "\n",
    "# Train the Gradient Boosting model\n",
    "gradient_boosting_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Extract feature importances\n",
    "feature_importances = gradient_boosting_pipeline.named_steps['model'].feature_importances_\n",
    "\n",
    "# Get feature names after one-hot encoding from the pipeline\n",
    "one_hot_transformer = gradient_boosting_pipeline.named_steps['preprocessor'].named_transformers_['one_hot']\n",
    "feature_names = one_hot_transformer.get_feature_names_out(input_features=categorical_features)\n",
    "other_feature_names = X.drop(categorical_features, axis=1).columns\n",
    "all_feature_names = np.concatenate([feature_names, other_feature_names])\n",
    "\n",
    "# Aggregate feature importances\n",
    "aggregated_importances = {}\n",
    "for name, importance in zip(all_feature_names, feature_importances):\n",
    "    key = 'day' if 'day' in name else 'meal_type' if 'meal_type' in name else name\n",
    "    aggregated_importances[key] = aggregated_importances.get(key, 0) + importance\n",
    "\n",
    "\n",
    "# Ensure the total sum of importances remains the same\n",
    "total_importance = sum(aggregated_importances.values())\n",
    "for key in aggregated_importances:\n",
    "    aggregated_importances[key] /= total_importance\n",
    "\n",
    "# Plotting adjusted aggregated feature importances\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(list(aggregated_importances.keys()), list(aggregated_importances.values()))\n",
    "plt.xlabel('Adjusted Aggregated Feature Importance')\n",
    "plt.ylabel('Feature Categories')\n",
    "plt.title('Adjusted Aggregated Feature Importances in Gradient Boosting Model')\n",
    "\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('footfall_735.csv')\n",
    "\n",
    "# Preprocessing\n",
    "categorical_features = ['day', 'meal_type']\n",
    "one_hot = OneHotEncoder()\n",
    "transformer = ColumnTransformer([(\"one_hot\", one_hot, categorical_features)], remainder=\"passthrough\")\n",
    "\n",
    "# Define target and features\n",
    "X = data.drop(['footfall', 'DATE', 'month'], axis=1)\n",
    "y = data['footfall']\n",
    "\n",
    "# Splitting data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a pipeline for Gradient Boosting\n",
    "gradient_boosting_pipeline = Pipeline([\n",
    "    ('preprocessor', transformer),\n",
    "    ('model', GradientBoostingRegressor())\n",
    "])\n",
    "\n",
    "# Hyperparameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'model__learning_rate': [0.1, 0.05, 0.01],\n",
    "    'model__n_estimators': [100, 200, 300],\n",
    "    'model__max_depth': [3, 4, 5]\n",
    "}\n",
    "\n",
    "# Perform GridSearchCV for hyperparameter tuning\n",
    "grid_search = GridSearchCV(gradient_boosting_pipeline, param_grid, cv=5, n_jobs=-1, verbose=1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and model\n",
    "best_params = grid_search.best_params_\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Extract feature importances from the best model\n",
    "feature_importances = best_model.named_steps['model'].feature_importances_\n",
    "\n",
    "\n",
    "\n",
    "# Visualization (Plotting adjusted aggregated feature importances)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(list(aggregated_importances.keys()), list(aggregated_importances.values()))\n",
    "plt.xlabel('Adjusted Aggregated Feature Importance')\n",
    "plt.ylabel('Feature Categories')\n",
    "plt.title('Adjusted Aggregated Feature Importances in Gradient Boosting Model')\n",
    "\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()\n",
    "\n",
    "# Additional code for displaying the best parameters from GridSearchCV\n",
    "print(\"Best Parameters:\", best_params)\n",
    "# Fit the best model obtained from GridSearchCV\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions using the best model\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Create a DataFrame for comparison\n",
    "comparison_df = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})\n",
    "\n",
    "# Plotting predicted vs actual values\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(comparison_df['Actual'], comparison_df['Predicted'], alpha=0.6)\n",
    "plt.plot(comparison_df['Actual'], comparison_df['Actual'], color='red', label='Actual = Predicted')\n",
    "\n",
    "plt.xlabel('Actual Footfall')\n",
    "plt.ylabel('Predicted Footfall')\n",
    "plt.title('Actual vs Predicted Footfall')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Additional code for displaying the best parameters from GridSearchCV\n",
    "print(\"Best Parameters:\", best_params)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.linear_model import PoissonRegressor\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('footfall_735.csv')\n",
    "\n",
    "# Preprocessing\n",
    "categorical_features = ['day', 'meal_type']\n",
    "one_hot = OneHotEncoder()\n",
    "transformer = ColumnTransformer([(\"one_hot\", one_hot, categorical_features)], remainder=\"passthrough\")\n",
    "\n",
    "# Define target and features\n",
    "X = data.drop(['footfall', 'DATE', 'month'], axis=1)\n",
    "y = data['footfall']\n",
    "\n",
    "# Splitting data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a pipeline for Gradient Boosting\n",
    "gradient_boosting_pipeline = Pipeline([\n",
    "    ('preprocessor', transformer),\n",
    "    ('model', GradientBoostingRegressor())\n",
    "])\n",
    "\n",
    "# Train the Gradient Boosting model\n",
    "gradient_boosting_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict footfall using Gradient Boosting\n",
    "gb_predictions = gradient_boosting_pipeline.predict(X_test)\n",
    "\n",
    "# Train Poisson Regression on the same data\n",
    "poisson_pipeline = Pipeline([\n",
    "    ('preprocessor', transformer),\n",
    "    ('model', PoissonRegressor(max_iter=1000))\n",
    "])\n",
    "\n",
    "poisson_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict footfall using Poisson Regression\n",
    "poisson_predictions = poisson_pipeline.predict(X_test)\n",
    "\n",
    "# Combine predictions\n",
    "combined_predictions = (gb_predictions + poisson_predictions) / 2\n",
    "\n",
    "# Calculate MAE for both models and combined predictions\n",
    "gb_mae = mean_absolute_error(y_test, gb_predictions)\n",
    "poisson_mae = mean_absolute_error(y_test, poisson_predictions)\n",
    "combined_mae = mean_absolute_error(y_test, combined_predictions)\n",
    "\n",
    "print(f\"Gradient Boosting MAE: {gb_mae}\")\n",
    "print(f\"Poisson Regression MAE: {poisson_mae}\")\n",
    "print(f\"Combined Model MAE: {combined_mae}\")\n",
    "\n",
    "# Visual comparison of the predictions\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(y_test.values, label='Actual Footfall', alpha=0.7)\n",
    "plt.plot(gb_predictions, label='Gradient Boosting Predictions', alpha=0.7)\n",
    "plt.plot(poisson_predictions, label='Poisson Predictions', alpha=0.7)\n",
    "plt.plot(combined_predictions, label='Combined Predictions', alpha=0.7)\n",
    "plt.legend()\n",
    "plt.title('Comparison of Footfall Predictions')\n",
    "plt.xlabel('Test Samples')\n",
    "plt.ylabel('Footfall')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "# Prepare the data for the model\n",
    "X = df.drop(['footfall', 'DATE'], axis=1)\n",
    "y = df['footfall']\n",
    "\n",
    "# One-hot encode the categorical variables\n",
    "\n",
    "one_hot = OneHotEncoder(handle_unknown='ignore')\n",
    "X_encoded = one_hot.fit_transform(X[categorical_features]).toarray()\n",
    "# Updated method to get feature names\n",
    "X_encoded = pd.DataFrame(X_encoded, columns=one_hot.get_feature_names_out(categorical_features))\n",
    "X = X.drop(categorical_features, axis=1).reset_index(drop=True)\n",
    "X_encoded = pd.concat([X_encoded, X], axis=1)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Building the neural network model\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_absolute_error')\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=30, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Plotting the training and validation loss\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss During Training')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Predicting the values using the model\n",
    "# Replace 'model', 'X_test', and 'y_test' with your actual model and data variables\n",
    "y_pred = model.predict(X_test).flatten()\n",
    "\n",
    "# Creating a DataFrame to compare actual vs predicted values\n",
    "comparison_df = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})\n",
    "mae = np.mean(np.abs(comparison_df['Actual'] - comparison_df['Predicted']))\n",
    "print(\"Mean Absolute Error (MAE):\", mae)\n",
    "\n",
    "# Plotting the real values vs predicted values\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.scatter(comparison_df['Actual'], comparison_df['Predicted'], alpha=0.6)\n",
    "plt.plot(comparison_df['Actual'], comparison_df['Actual'], color='red')\n",
    "plt.xlabel('Actual Footfall')\n",
    "plt.ylabel('Predicted Footfall')\n",
    "plt.title('Actual vs Predicted Footfall')\n",
    "plt.show()\n",
    "\n",
    "# Output the DataFrame for further inspection\n",
    "print(comparison_df.head())\n",
    "from sklearn.metrics import r2_score\n",
    "# Calculating R-squared\n",
    "r_squared = r2_score(comparison_df['Actual'], comparison_df['Predicted'])\n",
    "print(\"R-squared:\", r_squared)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
