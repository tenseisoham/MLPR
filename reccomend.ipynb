{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('menuxfootfall.csv')\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Concatenate values from the four columns into a new column 'merged_column'\n",
    "df['menu'] = df.apply(lambda row: ''.join(str(row[col]) for col in ['breakfast_merge', 'lunch_merge', 'snacks_merge', 'dinner_merge']), axis=1)\n",
    "\n",
    "# Display the new 'merged_column'\n",
    "\n",
    "# Function to clean and split menu items\n",
    "def clean_and_split_menu(menu_string):\n",
    "    # Replace 'nan' with an empty string and strip extra spaces\n",
    "    clean_string = menu_string.replace('nan', '').strip()\n",
    "    # Split the menu items and remove extra spaces\n",
    "    menu_items = [item.strip() for item in clean_string.split(',') if item.strip()]\n",
    "    return menu_items\n",
    "\n",
    "# Clean and split the 'menu' column\n",
    "df['menu'] = df['menu'].astype(str)  # Ensure all entries are strings\n",
    "menu_lists = df['menu'].apply(clean_and_split_menu)\n",
    "\n",
    "# Create a list of all unique menu items\n",
    "all_menu_items = set(item for sublist in menu_lists for item in sublist)\n",
    "\n",
    "# Initialize a dictionary to hold our one-hot encoded data\n",
    "one_hot_encoded_data = {item: [] for item in all_menu_items}\n",
    "\n",
    "# Populate the dictionary with one-hot encoded values for each row\n",
    "for menu_items in menu_lists:\n",
    "    for item in all_menu_items:\n",
    "        one_hot_encoded_data[item].append(int(item in menu_items))\n",
    "\n",
    "# Convert the dictionary to a DataFrame\n",
    "one_hot_encoded_df = pd.DataFrame(one_hot_encoded_data)\n",
    "\n",
    "# Combine the original DataFrame with the new one-hot encoded DataFrame\n",
    "final_df = pd.concat([df, one_hot_encoded_df], axis=1)\n",
    "\n",
    "# Now `final_df` contains the original data along with one-hot encoded menu items\n",
    "print(final_df['menu'].head())\n",
    "final_df.to_csv('columns.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('columns.csv')\n",
    "\n",
    "# Prepare the data for the model, dropping all non-numeric and target columns\n",
    "drop_columns = ['Date', 'breakfast_merge', 'snacks_merge', 'lunch_merge', 'dinner_merge', 'menu', 'footfall']\n",
    "X = df.drop(columns=drop_columns)\n",
    "y = df['footfall']\n",
    "\n",
    "# Handling categorical variables\n",
    "categorical_features = ['day', 'month', 'meal_type']\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Define which columns are numeric for the ColumnTransformer to work on\n",
    "numeric_features = X.columns.difference(categorical_features).tolist()\n",
    "\n",
    "# Combine categorical and numerical transformations\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', categorical_transformer, categorical_features),\n",
    "        ('num', StandardScaler(), numeric_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Apply preprocessing, which now only scales numeric features and one-hot encodes categorical features\n",
    "X_train = preprocessor.fit_transform(X_train)\n",
    "X_test = preprocessor.transform(X_test)\n",
    "\n",
    "# The input shape is the number of features after transformation\n",
    "input_shape = X_train.shape[1]\n",
    "\n",
    "# Building the neural network model\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(input_shape,)),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the model with validation split\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Plotting the training and validation loss\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss During Training')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Save the final model\n",
    "#model.save('final_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('menuxfootfall.csv')\n",
    "\n",
    "\n",
    "# This step is crucial as PCA is sensitive to the variances of the initial variables\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(df.select_dtypes(include=[np.number]))\n",
    "\n",
    "# Applying PCA to retain 95% of the variance\n",
    "pca = PCA(n_components=0.95)\n",
    "pca_data = pca.fit_transform(scaled_data)\n",
    "\n",
    "# Convert the PCA results into a DataFrame\n",
    "pca_df = pd.DataFrame(data=pca_data)\n",
    "\n",
    "\n",
    "# Save the PCA-reduced data to a new CSV file if needed\n",
    "pca_df.to_csv('pcad.csv', index=False)\n",
    "\n",
    "print(pca_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, PCA\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('menuxfootfall.csv')\n",
    "\n",
    "# Drop non-numeric columns and the target variable\n",
    "drop_columns = ['Date', 'breakfast_merge', 'snacks_merge', 'lunch_merge', 'dinner_merge', 'menu']\n",
    "X = df.drop(columns=drop_columns)\n",
    "y = df['footfall']\n",
    "\n",
    "# Standardize the data before applying PCA\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(X)\n",
    "\n",
    "# Applying PCA\n",
    "pca = PCA(n_components=0.95)  # Keep 95% of variance\n",
    "X_pca = pca.fit_transform(scaled_data)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Building the neural network model\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the model with validation split\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Plotting the training and validation loss\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss During Training')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('menuxfootfall.csv')\n",
    "\n",
    "# Drop non-numeric columns and the target variable\n",
    "drop_columns = ['Date', 'breakfast_merge', 'snacks_merge', 'lunch_merge', 'dinner_merge']\n",
    "X = df.drop(columns=drop_columns)\n",
    "y = df['footfall']\n",
    "\n",
    "# Standardize the data before applying PCA\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(X)\n",
    "\n",
    "# Applying PCA\n",
    "pca = PCA(n_components=0.95)  # Keep 95% of variance\n",
    "X_pca = pca.fit_transform(scaled_data)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Building the neural network model\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the model with validation split\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Plotting the training and validation loss\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss During Training')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
